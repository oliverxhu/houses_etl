{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1, 1, 1], [1, 1, 2], [2, 2, 2], [3, 3, 3]], columns=['id_region', 'id_district', 'id_suburb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user='housingdata'; passwd='housingdata123'; host='housing.ct0tluqftf3s.ap-southeast-2.rds.amazonaws.com'\n",
    "db='housing'\n",
    "engine = create_engine('postgresql+psycopg2://%s:%s@%s:5432/%s' % (\n",
    "            user, passwd, host, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x19019ee1278>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute('set search_path to staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_sql(\"select * from control where type = 'pk'\", con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_cols = ['photo_main', \t'end_date', \t'start_date', \t'note_date', \t'is_classified', \t'id_listing_tm', \t'amenities', \t'available_from', \t'bathrooms', \t'bedrooms', \t'best_contact_time', \t'ideal_tenant', \t'listing_group', \t'max_tenants', \t'parking', \t'pets_okay', \t'price_display', \t'property_id', \t'property_type', \t'smokers_okay', \t'start_price', \t'title', \t'rent_per_week', \t'whiteware', \t'address', \t'latitude', \t'longitude', \t'easting', \t'northing', \t'agency_reference', \t'category', \t'category_path', \t'has_embedded_video', \t'has_gallery', \t'is_bold', \t'is_featured', \t'is_highlighted', \t'is_super_featured', \t'listing_length', \t'reserve_state', \t'is_boosted', \t'id_agency_tm', \t'id_suburb_tm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tbl = pd.read_sql(\"SELECT %s FROM housing.rental_listings WHERE is_current = 1\" % (','.join(join_cols)), con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing = df_tbl.copy()\n",
    "staging = rl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_cols1 = [x for x in join_cols if x not in ('end_date')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.merge(left=staging, right=housing, how='left', on=join_cols1, suffixes=('', '_right'), indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing['latitude'] = housing['latitude'].astype('float64')\n",
    "housing['longitude'] = housing['longitude'].astype('float64')\n",
    "housing['easting'] = housing['easting'].astype('float64')\n",
    "housing['northing'] = housing['northing'].astype('float64')\n",
    "staging['start_date'] = staging['start_date'].astype('datetime64[ns, UTC]')\n",
    "staging['end_date'] = staging['end_date'].astype('datetime64[ns, UTC]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing['note_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'datetime64[ns, UTC]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in join_cols:\n",
    "    df = pd.merge(left=staging, right=housing, how='left', on=col, suffixes=('', '_right'), indicator=True)\n",
    "    if len(df[df['_merge']=='left_only']) > 0:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "staging.start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing['end_date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staging['end_date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in housing.columns:\n",
    "    print(col,': ', housing[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for date in ('end_date', 'start_date', 'note_date'):\n",
    "    df[date] = df[date].astype('datetime64[ns]')\n",
    "    df_tbl[date] = df_tbl[date].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfm = pd.merge(left=df, right=df_tbl, how='left', left_on=join_cols, right_on=join_cols, suffixes=('', '_tbl'), indicator=True)\n",
    "print(dfm['_merge'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['end_date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['end_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['rent_per_week'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfm[dfm['_merge']==['left_only']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = con.execute('Select \"ListingId\", \"PhotoUrls_0\", \t\"PhotoUrls_1\", \t\"PhotoUrls_10\", \t\"PhotoUrls_11\", \t\"PhotoUrls_12\", \t\"PhotoUrls_13\", \t\"PhotoUrls_14\", \t\"PhotoUrls_15\", \t\"PhotoUrls_16\", \t\"PhotoUrls_17\", \t\"PhotoUrls_18\", \t\"PhotoUrls_2\", \t\"PhotoUrls_3\", \t\"PhotoUrls_4\", \t\"PhotoUrls_5\", \t\"PhotoUrls_6\", \t\"PhotoUrls_7\", \t\"PhotoUrls_8\", \t\"PhotoUrls_9\", \t\"PictureHref\" FROM staging.tm_rental_listings_hourly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('connections.json') as file:\n",
    "    web_con = json.load(file)\n",
    "letl = Listings(connect_data=True, dt_user=web_con['dt_user'], dt_passwd=web_con['dt_passwd'],\n",
    "                dt_host=web_con['dt_host'], dt_db=web_con['dt_db'], dt_schema=web_con['dt_schema'],\n",
    "                connect_staging=True, st_user=web_con['st_user'], st_passwd=web_con['st_passwd'],\n",
    "                st_host=web_con['st_host'], st_db=web_con['st_db'], st_schema=web_con['st_schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rl = letl.rental_listings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import ETLTools\n",
    "import time\n",
    "import sqlalchemy\n",
    "\n",
    "\n",
    "class Listings(ETLTools.ETLTools):\n",
    "    \"\"\"\n",
    "    1a. Check Listing ID for new rows\n",
    "    1b. Populate ALL tables with new listing information\n",
    "    2a. Check active rows in website tables that aren't in staging\n",
    "    2b. Set is_current to False\n",
    "    3a. Check for changes to existing rows\n",
    "    3b. For existing rows that changed, change the is_current, valid_from, valid_to and version\n",
    "    3c. Add in the changed rows in ALL tables with new timestamps and versions\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize Listings object\n",
    "        :param connect_staging: Boolean initialize object to connect to staging table\n",
    "        :param st_user: staging username\n",
    "        :param st_passwd: staging password\n",
    "        :param st_host: staging host\n",
    "        :param st_db: staging database\n",
    "        :param st_schema: staging schema\n",
    "        :param connect_website: Same as above, except connect to the website. Params are prefixed with 'ws'\n",
    "        \"\"\"\n",
    "        # connection object for staging\n",
    "        if kwargs.get('connect_staging', None):\n",
    "            self.con_staging = ETLTools.DatabaseConnection(\n",
    "                kwargs['st_user'], kwargs['st_passwd'], kwargs['st_host'], kwargs['st_db'], kwargs['st_schema'])\n",
    "        # connection object for website\n",
    "        if kwargs.get('connect_data', None):\n",
    "            self.con_data = ETLTools.DatabaseConnection(\n",
    "                kwargs['dt_user'], kwargs['dt_passwd'], kwargs['dt_host'], kwargs['dt_db'], kwargs['dt_schema'])\n",
    "\n",
    "    \"\"\"Generic Functions\"\"\"\n",
    "    def _control(self, table):\n",
    "        \"\"\"\n",
    "        Returns a dataframe of the staging.control table\n",
    "        :param table: Table name in control table\n",
    "        :return: pandas DataFrame of staging.control\n",
    "        \"\"\"\n",
    "        return self.generator_to_df(self.con_staging.query(\n",
    "            \"SELECT source_col_name, dest_col_name, \\\"type\\\" FROM control WHERE dest_table_name = '%s'\" % table),\n",
    "            columns=['source_col_name', 'dest_col_name', 'type'])\n",
    "\n",
    "    def _control_mapping_columns(self, table):\n",
    "        \"\"\"\n",
    "        Returns source and destination mapping columns of table \n",
    "        :param table: relevant destination table\n",
    "        :return: DICTIONARY of source and destination mapping lists\n",
    "        \"\"\"\n",
    "        df = self._control(table)\n",
    "        source_mapping = list(df[df['type'] == 'mapping']['source_col_name'])\n",
    "        destination_mapping = list(df[df['type'] == 'mapping']['dest_col_name'])\n",
    "        return {'source_mapping': source_mapping, 'destination_mapping': destination_mapping}\n",
    "\n",
    "    def _control_source_columns(self, table, return_as_str=False):\n",
    "        \"\"\"\n",
    "        Finds the source columns for any given table\n",
    "        :param table: destination table\n",
    "        :param return_as_str: return as a string separated by \",\" commas\n",
    "        :return: LIST of columns in the source table\n",
    "        \"\"\"\n",
    "        if return_as_str:\n",
    "            return ', '.join(list(self._control(table).dropna()['source_col_name']))\n",
    "        return list(self._control(table).dropna()['source_col_name'])\n",
    "\n",
    "    def _control_destination_columns(self, table, key=True, meta=True, mapping=True):\n",
    "        \"\"\"\n",
    "        Finds the destination columns for any given table. Note this DOES NOT return metadata fields.\n",
    "        :param table: destination table\n",
    "        :param key: boolean to include the primary key of the destination table in output list\n",
    "        :param meta: boolean to include metadata columns\n",
    "        :param mapping: boolean to pull the mapping columns\n",
    "        :return: LIST of columns in the destination table\n",
    "        \"\"\"\n",
    "\n",
    "        df = self._control(table)\n",
    "        if not key:\n",
    "            df = df[df['type'] != 'pk']\n",
    "        if not meta:\n",
    "            df = df[df['type'] != 'metadata']\n",
    "        if not mapping:\n",
    "            df = df[df['type'] != 'mapping']\n",
    "        return list(df['dest_col_name'])\n",
    "\n",
    "    def _control_date_cols(self, table, kind='dest_col_name'):\n",
    "        date_ctrl = self._control(table)\n",
    "        return list(date_ctrl[date_ctrl['type'] == 'datetime'][kind])\n",
    "\n",
    "    def convert_ms_dates(self, df, table):\n",
    "        date_cols = self._control_date_cols(table)\n",
    "        for cols in date_cols:\n",
    "            df[cols] = df[cols].apply(lambda x: self.from_ms_timestamp(int(x[x.find(\"(\")+1:x.find(\")\")])))\n",
    "        return df\n",
    "\n",
    "    def convert_dtype_dates(self, df, table):\n",
    "        date_cols = self._control_date_cols(table)\n",
    "        for cols in date_cols:\n",
    "            df[cols] = df[cols].astype('datetime64[ns]')\n",
    "        return df\n",
    "\n",
    "    \"\"\"Table Update Functions\"\"\"\n",
    "    def update_agency(self, table='agency'):\n",
    "        \"\"\"Updates the agency table\"\"\"\n",
    "        source_agency_cols = self._control_source_columns(table, return_as_str=True)\n",
    "        destination_agency_cols = self._control_destination_columns(table, key=False, meta=False, mapping=True)\n",
    "        dest_map = self._control_mapping_columns(table=table)['destination_mapping']\n",
    "        agency_df = self.generator_to_df(self.con_staging.query(\"SELECT %s FROM tm_rental_listings_hourly\"\n",
    "                                                                % source_agency_cols), columns=destination_agency_cols)\n",
    "        agency_df = agency_df.dropna(subset=['id_agency_tm'])\n",
    "        agency_df = agency_df.drop_duplicates(subset=['id_agency_tm'])\n",
    "        agency_df = agency_df.assign(row_insert_date=self.datenow())\n",
    "\n",
    "        agency_df = self.convert_ms_dates(agency_df, table)\n",
    "        type1 = [x for x in destination_agency_cols if x != 'id_agency_tm']\n",
    "        self.con_data.update_scd_type_one(df=agency_df, dimension_table=table, key='id_agency',\n",
    "                                          attributeslist=destination_agency_cols + ['row_insert_date'],\n",
    "                                          lookupatts=dest_map,\n",
    "                                          type1atts=type1 + ['row_insert_date']\n",
    "                                          )\n",
    "\n",
    "    def rental_listings(self, table='rental_listings'):\n",
    "        source_cols = self._control_source_columns(table=table)\n",
    "        destination_cols = self._control_destination_columns(table, key=False, meta=False,\n",
    "                                                             mapping=True)\n",
    "        date_cols = self._control_date_cols(table)\n",
    "\n",
    "        staging_listings_df = self.generator_to_df(self.con_staging.query('SELECT %s FROM tm_rental_listings_hourly'\n",
    "                                                   % ', '.join(source_cols)), columns=destination_cols)\n",
    "        staging_listings_df = staging_listings_df.assign(is_current=1, valid_from=self.datenow(),\n",
    "                                                         valid_to=self.dateend(), row_insert_date=self.datenow())\n",
    "        staging_listings_df = self.convert_ms_dates(staging_listings_df, table)\n",
    "        return staging_listings_df\n",
    "    \n",
    "    def update_rental_listings(self, table='rental_listings'):\n",
    "        \"\"\"Completes jobs for rental_listings\"\"\"\n",
    "        \"\"\"Issues\n",
    "            Update Existing records is not joining properly to the table\n",
    "        \"\"\"\n",
    "        source_cols = self._control_source_columns(table=table)\n",
    "        destination_cols = self._control_destination_columns(table, key=False, meta=False,\n",
    "                                                             mapping=True)\n",
    "        date_cols = self._control_date_cols(table)\n",
    "\n",
    "        staging_listings_df = self.generator_to_df(self.con_staging.query('SELECT %s FROM tm_rental_listings_hourly'\n",
    "                                                   % ', '.join(source_cols)), columns=destination_cols)\n",
    "        staging_listings_df = staging_listings_df.assign(is_current=1, valid_from=self.datenow(),\n",
    "                                                         valid_to=self.dateend(), row_insert_date=self.datenow())\n",
    "        staging_listings_df = self.convert_ms_dates(staging_listings_df, table)\n",
    "\n",
    "        print('adding new listings...')\n",
    "\n",
    "        # 1) Add new records\n",
    "        self.con_data.add_new_records(df=staging_listings_df, df_lookup_column_list=['id_listing_tm'],\n",
    "                                      table=table, table_lookup_column_list=['id_listing_tm'])\n",
    "        print('records total: %s' % [x for x in self.con_data.query(\"select count(*) from rental_listings\")])\n",
    "        print('deactivating old listings...')\n",
    "\n",
    "        staging_listings_df.to_csv(\"staging_listings_df.csv\", index=None)\n",
    "\n",
    "        # 2) deactivate columns that aren't active anymore (ie. in datalayer but not staging)\n",
    "        # this is changing everything to old\n",
    "        old_record_list = self.con_data.find_old_records(df=staging_listings_df,\n",
    "                                                         df_lookup_column_list=['id_listing_tm'],\n",
    "                                                         table=table,\n",
    "                                                         table_lookup_column_list=['id_listing_tm'],\n",
    "                                                         table_pk='id_rental_listing',\n",
    "                                                         convert_datetime_cols=date_cols)\n",
    "        if len(old_record_list) != 0:\n",
    "            self.con_data.query(\"UPDATE rental_listings SET is_current=0, valid_to='%s' WHERE id_rental_listing IN(%s)\"\n",
    "                                % (self.dateend(), ', '.join(old_record_list)))\n",
    "\n",
    "        print('after deactivation: %s' % [x for x in self.con_data.query(\"select count(*) from rental_listings\")])\n",
    "        print('updating existing listings...')\n",
    "\n",
    "        # 3) update existing records\n",
    "        update_df = self.con_data.table_lookup(df=staging_listings_df,\n",
    "                                               df_lookup_column_list=['id_listing_tm'],  # destination_cols,\n",
    "                                               table=table,\n",
    "                                               table_lookup_column_list=['id_listing_tm'],  # destination_cols,\n",
    "                                               table_return_column_list=[],\n",
    "                                               wheresql=\"WHERE is_current = 1\",\n",
    "                                               convert_datetime_cols=date_cols)\n",
    "        print('new rows to write as version differences: %s' % len(update_df))\n",
    "        update_df.to_csv(\"update_df.csv\", index=None)\n",
    "        update_df = update_df[update_df['_merge'] == 'left_only']\n",
    "        update_df.to_csv(\"update_df_left_only.csv\", index=None)\n",
    "\n",
    "        if not len(update_df) == 0:\n",
    "            # update old rows\n",
    "            print('updating old rows...')\n",
    "            for ind, row in update_df.iterrows():\n",
    "                self.con_data.query(\"\"\"UPDATE %s\n",
    "                                       SET is_current = 0, valid_to = '%s' \n",
    "                                       WHERE id_listing_tm = '%s'\n",
    "                                       AND is_current = 1\n",
    "                                       \"\"\" % (table, self.datenow(), row['id_listing_tm']))\n",
    "            # write new rows\n",
    "            update_df = update_df[staging_listings_df.columns]\n",
    "\n",
    "            self.con_data.append_df_to_table(df=update_df, table=table)\n",
    "\n",
    "\n",
    "\n",
    "    def update_agent_rental(self, table='agent_rental'):\n",
    "        \"\"\"Updates the agent_rental table\"\"\"\n",
    "        agent_source_cols = ['\"ListingId\"', '\"Agency_Agents_0_FullName\"', '\"Agency_Agents_0_MobilePhoneNumber\"',\n",
    "                             '\"Agency_Agents_0_OfficePhoneNumber\"', '\"Agency_Agents_0_Photo\"', '\"Agency_Agents_0_UrlSlug\"']\n",
    "        # destination_cols = self._control_destination_columns(table, key=False, meta=False, mapping=True)\n",
    "        destination_cols = ['id_rental_listing', 'fullname', 'mobile', 'office_phone', 'photo', 'url_slug']\n",
    "        mapping_cols = self._control_mapping_columns(table)\n",
    "        destination_key = self._control(table)[self._control(table)['type'] == 'pk']['dest_col_name']\n",
    "\n",
    "        agent_rental_df = pd.DataFrame(columns=destination_cols)\n",
    "        for i in range(2):\n",
    "            pull_cols = [x.replace('0', str(i)) for x in agent_source_cols]\n",
    "            df = self.generator_to_df(self.con_staging.query('SELECT %s FROM tm_rental_listings_hourly' % ', '\n",
    "                                                             .join(pull_cols)), columns=destination_cols)\n",
    "            df = df[(df['fullname'] != 'None')]\n",
    "            df = df.dropna(subset=['fullname'])\n",
    "            agent_rental_df = pd.concat([agent_rental_df, df])\n",
    "\n",
    "        agent_rental_df = agent_rental_df.assign(row_insert_date=self.datenow())\n",
    "        agent_rental_df = self.convert_ms_dates(agent_rental_df, table)\n",
    "        atts = destination_cols + ['row_insert_date']\n",
    "        # set agent_rental as a SCD type 1\n",
    "        self.con_data.update_scd_type_one(agent_rental_df, table, key=destination_key[0],\n",
    "                                          attributeslist=atts, lookupatts=['id_rental_listing'],\n",
    "                                          type1atts=[x for x in atts if x not in mapping_cols['destination_mapping']])\n",
    "\n",
    "    def update_rental_photo(self, table='rental_photo'):\n",
    "        \"\"\"Updates the rental_photos table\"\"\"\n",
    "        # get source cols as list, and compare with long table of photo keyshe wont\n",
    "        source_cols = self._control_source_columns(table)\n",
    "        mapping_cols = self._control_mapping_columns(table)\n",
    "        photo_cols = [x for x in source_cols if x not in ['\"ListingId\"', '\"PictureHref\"']]\n",
    "        photo_df = self.generator_to_df(\n",
    "            self.con_staging.query(\"SELECT %s FROM tm_rental_listings_hourly\" % ', '.join(source_cols)),\n",
    "            columns=source_cols)\n",
    "\n",
    "        photo_df = pd.melt(photo_df, id_vars=mapping_cols['source_mapping'], value_vars=photo_cols,\n",
    "                           var_name='photo_cols', value_name='photo_url')\n",
    "        photo_df = photo_df.rename(columns={'\"ListingId\"': 'id_rental_listing'})\n",
    "        photo_df = photo_df.drop('photo_cols', axis=1)\n",
    "        photo_df = photo_df.dropna(subset=['photo_url'])\n",
    "        photo_df = photo_df.assign(row_insert_date=self.datenow())\n",
    "        photo_df = self.convert_ms_dates(photo_df, table)\n",
    "\n",
    "        print('updating rental_photo...')\n",
    "        self.con_data.add_new_records(photo_df, df_lookup_column_list=['id_rental_listing', 'photo_url'],\n",
    "                                      table=table, table_lookup_column_list=['id_rental_listing', 'photo_url'])\n",
    "\n",
    "\n",
    "def run_listings():\n",
    "    with open('connections.json') as file:\n",
    "        web_con = json.load(file)\n",
    "    letl = Listings(connect_data=True, dt_user=web_con['dt_user'], dt_passwd=web_con['dt_passwd'],\n",
    "                    dt_host=web_con['dt_host'], dt_db=web_con['dt_db'], dt_schema=web_con['dt_schema'],\n",
    "                    connect_staging=True, st_user=web_con['st_user'], st_passwd=web_con['st_passwd'],\n",
    "                    st_host=web_con['st_host'], st_db=web_con['st_db'], st_schema=web_con['st_schema'])\n",
    "    letl.update_rental_listings()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
